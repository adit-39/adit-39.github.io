<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Voice Conversion</title>

		<meta name="description" content="A presentation on machine learning approaches for VC">
		<meta name="author" content="Aditya Dalwani">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->

        <script type="text/javascript" src="lib/js/jquery.min.js"></script>
        <script type="text/javascript">
        $(document).ready( function(){
          $('#male').click(function(){
               $('#a1').trigger('play');
             });
          $('#female').click(function(){
               $('#a2').trigger('play');
             });
          $('#converted').click(function(){
               $('#a3').trigger('play');
          });
        });
        </script>

	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">

				<header>
					<img src="img/PES_Logo.png" data-background-transition="fade" width='120' height='120'
						 style="border:3px solid black;  position:absolute; top:0px; left:1000px;" />
				</header>
				<footer style="position:absolute; bottom:1px; font-size:18px;">Dept. of Computer Science and Engg.</footer>

				<section data-background="#b5533c" data-background-transition="fade">
					<h1>Voice Conversion</h1>
					<h3>A look into various Machine Learning based solutions</h3>
					<p>
						<small> Seminar Guided by <a href="https://in.linkedin.com/in/ramamoorthy-srinath-644a969b">Dr. Ramamoorthy Srinath</a></small>
					</p>
					<p>
						<small>Seminar by <a href="https://in.linkedin.com/in/adityadalwani">Aditya Dalwani</a> | 1PI12CS011 </small>
					</p>
				</section>


				<!-- Example of nested vertical slides -->
				<section data-background="#4d7e65" data-background-transition="fade">
					<section>
						<h2>Presentation Structure</h2><br>
						<h3 style="font-style:italic; font-face:bold">1.  Introduction</h3>
						<ul>
							<li><p class="fragment grow">What is Voice Conversion?</p></li>
							<li><p class="fragment grow">What are the Goals of Voice Conversion?</p></li>
							<li><p class="fragment grow">Which Speech Related Applications depend on VC?</p></li>
							<li><p class="fragment grow">Potential Misuse of Voice Conversion</p></li>
						</ul>
					</section>
					<section>
						<h3 style="font-style:italic; font-face:bold">2.  Solution Timeline</h3>
						<ul>
                            <li><p class="fragment grow">A Typical Voice Conversion System</p></li>
							<li><p class="fragment grow">Evolution of Solutions</p></li>
                            <li><p class="fragment grow">Traditonal vs Modern Techniques</p></li>
						</ul>
					</section>
					<section>
						<h3 style="font-style:italic; font-face:bold">3.  Current State-Of-The-Art</h3>
						<ul>
							<li><p class="fragment grow"> Conception of the Solution </p></li>
							<li><p class="fragment grow"> Overview of Related Technology</p></li>
							<li><p class="fragment grow"> Solution </p></li>
							<li><p class="fragment grow"> Results </p></li>
							<li><p class="fragment grow"> Conclusion </p></li>
						</ul>
					</section>
				</section>

				<section data-background="#cc6600">
					<h2>Introduction</h2>
				</section>
                <section>
                    <h3>What is Voice Conversion?</h3>
                    <p>
                        <b><u>General Definition </u></b> : Technique to modify the speech signal of one speaker (source) to sound like that of another speaker (target).
                    </p>
                    <p>
                        <b><u>Extended </u></b> : To modify (transform) the <i>characteristics</i> of the speech signal. (Eg. changing emotion/ pronunciation)
                    </p>
                    <div>
                        <div class="fragment" style="float:left; padding:10px"><img id="male" width=200 height=250 src="img/spanish.jpg" /></div>
						<div style="float:left; padding:10px" id="wrap1"><audio id="a1" src="audio/T6B79330025.es1.wav" hidden="true" preload="auto"></audio></div>
                        <div class="fragment" style="float:left; padding:10px"><img id="female" width=200 height=250 src="img/lisa.png" /></div>
						<div style="float:left; padding:10px" id="wrap2"><audio id="a2" src="audio/T6B75330154.en.wav" hidden="true" preload="auto"></audio></div>
						<div class="fragment" style="float:right; padding:10px"><img id="converted" width=200 height=250 src="img/bart.jpg" /></div>
						<div style="float:left; padding:10px" id="wrap3"><audio id="a3" src="audio/SIE1_CVC_EN_75-79.cnv.08.wav" hidden="true" preload="auto"></audio></div>
                    </div>
                </section>

                <section>
                    <h3>What are the Goals of Voice Conversion?</h3>
                    <p>
                        <b>Ultimate Goal </b>: The exact same content spoken by the source should be emitted by the VC System in the target's so that it passes the Turing Test.
                    </p>
                    <p>
                        The smaller goals that a VC System has to solve is understanding <i>what</i> needs to be changed (Can be used specifically as well).
                        <ul>
                            <li>Spectrum</li>
                            <li>Prosody</li>
                            <li>Spectrum as well as Prosody</li>
                        </ul>
                    </p>
                </section>

                <section>
                    <h3>Applications around Voice Conversion</h3>
                    <ol>
                        <li> <u>Text-To-Speech Systems (TTS)</u> <br><p>In these systems, the content to be spoken is generated as text. Speech Synthesis is hard, and extending that to multiple voices would make it that much harder. Hence, generate one voice, convert to others.</p></li>
                        <li> <u>Application to Various Innovations</u> <br><p>With the rate of growth of consumers, especially in terms of smartphones, a variety of new and useful applications are starting to emerge in domain of voice and speech processing.</p></li>
                    </ol>
                </section>

				<section>
					<h2>Possible Misuse of VC</h2>

					<blockquote>
						&ldquo;With Great Power Comes Great Responsibility&rdquo;
					</blockquote>
                    <p>
                        One of the many techniques applicable in the voice domain is Voice Fingerprinting. This allows one to identify or profile individuals on the basis of their voice much like how normal fingerprints are used.
                    </p>
                    <p>
                        Voice Conversion could prove to be the key to the lock provided by Fingerprinting in security applications, rendering voice fingerprinting practically useless in that domain.
                    </p>
				</section>

                <section data-background="#cc6600">
                    <h2>Solution Timeline</h2>
                </section>

                <section>
                    <h3>A Typical VC System</h3>
                    <img src="img/VCSys.png" height="600" />
                </section>

                <section>
                    <h3>Evolution of VC Systems</h3>
                    <iframe src='//cdn.knightlab.com/libs/timeline3/latest/embed/index.html?source=1YH-TScUtxNmpIihjsTvb0m2bFoC7niswdmGKSE9g2ME&font=Default&lang=en&initial_zoom=2&height=650' width='100%' height='650' frameborder='0'></iframe>
                   <!--<div data-timeline="lib/timeline.json" style="width:'500px'"></div>-->
                </section>

                <section>
                    <h3>Traditional vs Modern Systems</h3>
                    <ul>
                        <p>An interesting lesson learnt from this timeline is with regards to the application of systems any ML problem.
                        A couple of decades ago, the major approaches to solve VC began with Vector Quantization (based on k-means clustering) in combination with statistical approaches.
                            Gradually, the approach shifted to Neural Nets and then to GMMs to capture the waveforms.
                            Finally, a more non-linear approach was built using Deep Belief Nets.
                        </p>
                        <p>
                            The above actually prove that at any instance in time, the emerging technologies are very potent in their ability to improve the performance of AI problems that haven't been satisfactorily solved before.
                        </p>
                    </ul>
                </section>

				<section data-background="#cc6600">
					<h2>Current State-of-the-Art</h2>
				</section>

                <section>
                    <h3>Conception</h3>
                    <ul>
                        <li>The approaches based on GMMs rely on "shallow" voice conversions.<br><p><i>Shallow:</i> methods are based on piecewise linear transformations</p></li>
                        <li>Two Hypothesis:
                            <ol>
                                <li>Human speech is more a non-linear entity, hence, a non-linear VC model is better suited to the problem of VC.</li>
                                <li>Discriminative models are more suited to this task than Generative models.</li>
                            </ol>
                        </li>
                        <li class="fragment">Proof?</li>
                    </ul>
                </section>

                <section>
                    <h3>Conception (Contd.)</h3>
                    <ul>
                        <li>GMM Based approaches often suffer from over-smoothing due to feature learning through Gaussian mixtures which cause outputs to fall near the mean of the resultant Gaussian.</li>
                        <li>These approaches also suffer from over-fitting as its easy to overshoot the optimum number of Gaussians to overfit training data while training.</li>
                        <li>The work on NN based Voice Conversion (presented in the timeline) is known to perform much better than the GMM based VC.</li>
                        <li>Although NNs may also suffer through problem of over-fitting, it can be avoided by carefully preparing the training data and tweaking hyperparameters.</li>
                    </ul>
                </section>

				<section>
					<h3>Conception (Contd.)</h3>
					<ul>
						<li>The authors decided to extend their previous work in which they used RBMs to learn higher order features to reduce the problem of over-fitting. A concatenating NN was used as well.</li>
						<li>They improved on the same work to systematically capture temporal as well as latent (deep) relationships between source and target speech by retaining the concatenating NN, but replacing the vanilla RBMs by Speaker Dependent Recurrent Temporal RBMs.</li>
					</ul>
				</section>

				<section>
					<h3>Let's Get More Technical!</h3>
					<ul>
						<li><div><span><u><i><b>Restricted Boltzmann Machines (RBMs):</b></i></u></span></div></li>
                        <li>Boltzmann Machines are a form of Markov Random Fields whose energy function is of a linear form.</li>
                        <li>Simple BMs are made more powerful by arranging them in such a manner that not all units have an observable component to them i.e. they are arranged in various hidden layers that represent latent behavior that influence the observables but are not themselves directly observable.</li>
					</ul>
				</section>

                <section>
                    <h3>Technical Background (Contd.)</h3>
                    <ul>
                        <li>These more powerful BMs will become hard to train if all connections are maintained. So, the connections are restricted such that all units from one layer are connected to all other units from another layer. Hence the name "Restricted Boltzmann Machines".</li>
                        <div><div style="width:80%; margin: 0 auto;"><img src="img/bm_rbm.jpg" width="650px" /></div></div>
                    </ul>
                </section>

                <section>
                    <h3>Technical Background (Contd.)</h3>
                    <p>So how is RBM different from a NN with hidden layers?</p>
                    <p class="fragment">RBMs are also Neural Nets! However, they are not interpreted as feedforward networks, but more like bipartite graphs where the idea is to learn the joint probability distribution of hidden and input variables.</p>
                    <p class="fragment">Not only this, RBMs have a different set of training algorithms. Furthermore, RBMs have the capability to learn representations and generate samples (Generative model), but the trained weights can also be used in the feedforward sense.</p>
                </section>

                <section>
                    <h3>Building up!</h3>
                    <ul>
 						<li><div><span><u><i><b>Temporal RBMs (TRBMs):</b></i></u></span></div></li>
                        <li>A TRBM is a sequence of RBMs arranged in such a way that at a given timestamp, the RBMs biases depend only on the state of the RBM at the previous timestep.</li>
                        <li>They are analogous to HMMs but with an infinitely large state space and compact emission and transition probabilities. However, they are exponentially expensive to train and hence exact inference is hard.</li>
                        <div><div style="width:50%; margin: 0 auto;"><img src="img/trbm.png" width="400px" /></div></div>
                    </ul>
                </section>

                <section>
                    <h3>Pre-Heating the Oven!</h3>
                    <ul>
                        <li><div><span><u><i><b>Recurrent Temporal RBMs (RTRBMs):</b></i></u></span></div></li>
                        <li>RTRBMs are an extension to TRBMs that are simple to train and infer from. They are designed much like Recurrent Neural Networks and have a set of weights from hidden of previous timestep to hidden of current timestep which make things easier in terms of the algorithmic design and complexity.</li>
                        <li>RTRBMs are therefore, not only capable of capturing the latent relationships in terms of hidden abstractions, but also can now capture latent temporal relationships, hence making them a good candidate for voice based complex tasks, such as the one for VC.</li>
                    </ul>
                </section>

                <section>
                    <h3>Recurrent Temporal Restricted Boltzmann Machines</h3>
                    <div>
                        <div style="float:left"><img width="500" src="img/rtrbm.png" /></div>
                        <div style="float:right"><img width="500" src="img/rtrbm_2.png" /></div>
                    </div>
                </section>

                <section>
                    <h3>Actual Solution</h3>
                    <ul>
                        <li>The architecture consists of individual Speaker-Dependant RTRBMs.</li>
                        <li>By this, it is meant that the RTRBMs are trained separately on each speaker beforehand.</li>
                    </ul>
                    <div>
                        <img src="img/archi1.png" align="right" height="250px"/>
                        <span style="padding:10px">1. The feature h<sub>x</sub><sup>(t)</sup> captures linguistic information</span><br/>
                        <span style="padding:10px">2. B, C and h together capture temporal information</span><br/>
                        <span style="padding:10px">3. W and h capture latent abstract information</span><br/>
                    </div>
                </section>

                <section>
                    <h3>Actual Solution</h3>
                    <ul>
                        <li></li>
                    </ul>
                </section>

				<section data-background="#b5533c">
					<h1>THE END</h1>
				</section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'concave', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'lib/js/reveal-timeline.js', condition: function(){ return !!document.querySelector( '[data-timeline]' );} } //, js: 'lib/js/timeline-min.js', css: 'lib/css/timeline.css'}
				]
			});

		</script>

	</body>
</html>
