<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Voice Conversion</title>

		<meta name="description" content="A presentation on machine learning approaches for VC">
		<meta name="author" content="Aditya Dalwani">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->

        <script type="text/javascript" src="lib/js/jquery.min.js"></script>
        <script type="text/javascript">
        $(document).ready( function(){
          $('#male').click(function(){
               $('#a1').trigger('play');
             });
          $('#female').click(function(){
               $('#a2').trigger('play');
             });
          $('#converted').click(function(){
               $('#a3').trigger('play');
          });
        });
        </script>

	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">

				<header>
					<img src="img/PES_Logo.png" data-background-transition="fade" width='120' height='120'
						 style="border:3px solid black;  position:absolute; top:0px; left:1000px;" />
				</header>
				<footer style="position:absolute; bottom:1px; font-size:18px;">Dept. of Computer Science and Engg.</footer>

				<section data-background="#b5533c" data-background-transition="fade">
					<h1>Voice Conversion</h1>
					<h3>A look into various Machine Learning based solutions</h3>
					<p>
						<small> Seminar Guided by <a href="https://in.linkedin.com/in/ramamoorthy-srinath-644a969b">Dr. Ramamoorthy Srinath</a></small>
					</p>
					<p>
						<small>Seminar by <a href="https://in.linkedin.com/in/adityadalwani">Aditya Dalwani</a> | 1PI12CS011 </small>
					</p>
				</section>


				<!-- Example of nested vertical slides -->
				<section data-background="#4d7e65" data-background-transition="fade">
					<section>
						<h2>Presentation Structure</h2><br>
						<h3 style="font-style:italic; font-face:bold">1.  Introduction</h3>
						<ul>
							<li><p class="fragment grow">What is Voice Conversion?</p></li>
							<li><p class="fragment grow">What are the Goals of Voice Conversion?</p></li>
							<li><p class="fragment grow">Which Speech Related Applications depend on VC?</p></li>
							<li><p class="fragment grow">Potential Misuse of Voice Conversion</p></li>
						</ul>
					</section>
					<section>
						<h3 style="font-style:italic; font-face:bold">2.  Solution Timeline</h3>
						<ul>
                            <li><p class="fragment grow">A Typical Voice Conversion System</p></li>
							<li><p class="fragment grow">Evolution of Solutions</p></li>
                            <li><p class="fragment grow">Traditonal vs Modern Techniques</p></li>
						</ul>
					</section>
					<section>
						<h3 style="font-style:italic; font-face:bold">3.  Current State-Of-The-Art</h3>
						<ul>
							<li><p class="fragment grow"> Conception of the Solution </p></li>
							<li><p class="fragment grow"> Overview of Related Technology</p></li>
							<li><p class="fragment grow"> Solution </p></li>
							<li><p class="fragment grow"> Results </p></li>
							<li><p class="fragment grow"> Conclusion </p></li>
						</ul>
					</section>
				</section>

				<section data-background="#cc6600">
					<h2>Introduction</h2>
				</section>
                
                <section data-background="#004466">
                    <h3>What is Voice Conversion?</h3>
                    <p>
                        <b><u>General Definition </u></b> : Technique to modify the speech signal of one speaker (source) to sound like that of another speaker (target).
                    </p>
                    <p>
                        <b><u>Extended </u></b> : To modify (transform) the <i>characteristics</i> of the speech signal. (Eg. changing emotion/ pronunciation)
                    </p>
                    <div>
                        <div class="fragment" style="float:left; padding:10px"><img id="male" width=200 height=250 src="img/spanish.jpg" /></div>
						<div style="float:left; padding:10px" id="wrap1"><audio id="a1" src="audio/T6B79330025.es1.wav" hidden="true" preload="auto"></audio></div>
                        <div class="fragment" style="float:left; padding:10px"><img id="female" width=200 height=250 src="img/lisa.png" /></div>
						<div style="float:left; padding:10px" id="wrap2"><audio id="a2" src="audio/T6B75330154.en.wav" hidden="true" preload="auto"></audio></div>
						<div class="fragment" style="float:right; padding:10px"><img id="converted" width=200 height=250 src="img/bart.jpg" /></div>
						<div style="float:left; padding:10px" id="wrap3"><audio id="a3" src="audio/SIE1_CVC_EN_75-79.cnv.08.wav" hidden="true" preload="auto"></audio></div>
                    </div>
                </section>

                <section data-background="#004466">
                    <h3>What are the Goals of Voice Conversion?</h3>
                    <p>
                        <b>Ultimate Goal </b>: The exact same content spoken by the source should be emitted by the VC System in the target's so that it passes the Turing Test.
                    </p>
                    <p>
                        The smaller goals that a VC System has to solve is understanding <i>what</i> needs to be changed (Can be used specifically as well).
                        <ul>
                            <li>Spectrum</li>
                            <li>Prosody</li>
                            <li>Spectrum as well as Prosody</li>
                        </ul>
                    </p>
                </section>

                <section data-background="#004466">
                    <h3>Applications around Voice Conversion</h3>
                    <ol>
                        <li> <u>Text-To-Speech Systems (TTS)</u> <br><p>In these systems, the content to be spoken is generated as text. Speech Synthesis is hard, and extending that to multiple voices would make it that much harder. Hence, generate one voice, convert to others.</p></li>
                        <li> <u>Application to Various Innovations</u> <br><p>With the rate of growth of consumers, especially in terms of smartphones, a variety of new and useful applications are starting to emerge in domain of voice and speech processing.</p></li>
                    </ol>
                </section>

				<section data-background="#004466">
					<h2>Possible Misuse of VC</h2>

					<blockquote>
						&ldquo;With Great Power Comes Great Responsibility&rdquo;
					</blockquote>
                    <p>
                        One of the many techniques applicable in the voice domain is Voice Fingerprinting. This allows one to identify or profile individuals on the basis of their voice much like how normal fingerprints are used.
                    </p>
                    <p>
                        Voice Conversion could prove to be the key to the lock provided by Fingerprinting in security applications, rendering voice fingerprinting practically useless in that domain.
                    </p>
				</section>

                <section data-background="#cc6600">
                    <h2>Solution Timeline</h2>
                </section>

                <section data-background="#004466">
                    <h3>A Typical VC System</h3>
                    <img src="img/VCSys.png" height="600" />
                </section>

                <section data-background="#004466">
                    <section>
                        <h3>Evolution of VC Systems</h3>
                    </section>
                    <!--<iframe src='//cdn.knightlab.com/libs/timeline3/latest/embed/index.html?source=1YH-TScUtxNmpIihjsTvb0m2bFoC7niswdmGKSE9g2ME&font=Default&lang=en&initial_zoom=2&height=650' width='100%' height='650' frameborder='0'></iframe>-->
                   <!--<div data-timeline="lib/timeline.json" style="width:'500px'"></div>-->
                    <section>
                        <h3>1988: Voice Conversion Through Vector Quantization</h3>
                        <h5><i>Authors: Abe, Nakamura, Shikano, Kuwabara</i></h5>
                        <ul>
                            <li>Technique involved mapping voice codebooks (from Vector Quantization) between speakers using Dynamic Time Warping distance between the two time series.</li>
                            <li>The correspondences were accumulated as histograms which served as a weighting function.</li>
                            <li>This function would be applied on a linear combination of vectors to achieve the conversion. </li>
                        </ul>
                    </section>
                    <section>
                        <h3>1994: Transformation of Formants for Voice Conversion Using Artificial Neural Networks</h3>
                        <h5><i>Authors: Narendranath, Murthy, Rajendran, Yegnarayana</i></h5>
                        <ul>
                            <li>A formant is a concentration of acoustic energy around a particular frequency in the speech wave.
                            <li>System is a symmetric-looking ANN. Inputs are the source formants, outputs are the target formants.</li>
                            <li>Trained using the traditional backpropagation algorithm.</li>
                        </ul>
                    </section>
                    <section>
                        <h3>1998: Continuous Probabilistic Transform for Voice Conversion</h3>
                        <h5><i>Authors: Stylianou, Cappe, Moulines</i></h5>
                        <ul>
                            <li>Make use of GMM (Simplified ergodic HMM) which acts as a 'soft' classifier.</li>
                            <li>The actual conversion is done similar to how Abe et. al carried out their conversion, but components are now clustered into Gaussians rather than individually considered as in the VQ approach.</li>
                            <li>The parameters of the conversion function are determined by minimization of the total quadratic spectral distortion between the converted envelopes and the target envelopes. DTW is applied for this.</li>
                        </ul>
                    </section>
                    <section>
                        <h3>1998: Spectral Voice Conversion for Text-To-Speech Synthesis</h3>
                        <h5><i>Authors: Kain, Macon</i></h5>
                        <ul>
                            <li>Vector Quantize training data. Source and target vectors are aligned using DTW and GMMs were constructed and trained using EM algorithm.</li>
                            <li>Spectral info converted in parameterized way using GMM, Pitch adjusted using mean and variance of target pitch</li>
                        </ul>
                    </section>
                    <section>
                        <h3>2001: Voice Conversion Algorithm Based on GMM with Dynamic Frequency Warping of STRAIGHT Spectrum </h3>
                        <h5><i>Authors: Toda, Saruwatari, Shikano</i></h5>
                        <ul>
                            <li>In VC with GMMs, quality of converted speech is degraded as the spectrum is exceedingly smoothed.</li>
                            <li>Use DFW to avoid over-smoothing. "Residual" spectrum (difference between GMM and DFW spectra) avoids deterioration of conversion accuracy.</li>
                        </ul>
                    </section>
                    <section>
                        <h3>2005: Spectral Conversion Based on Maximum Likelihood Estimation Considering Global Variance of Converted Parameter </h3>
                        <h5><i>Authors: Toda, Black, Tokuda</i></h5>
                        <ul>
                            <li>Spectral Conversion using GMM.</li>
                            <li>Smooth Spectral sequence can be obtained from MLE using dyamic features to the GMM-based mapping, but oversmoothing will continue to be an issue due to ML based parameter estimation.</li>
                            <li>To avoid this, ML based conversion taking account global variance of the converted parameter in each utterance.</li>
                        </ul>
                    </section>
                    <section>
                        <h3>2013: Voice Conversion in High-order Eigen Space Using Deep Belief Nets</h3>
                        <h5><i>Authors: Nakashika, Takashima, Takiguchi, Ariki</i></h5>
                        <ul>
                            <li>The core idea was to shift conversion of features from Source to Target to the eigen space instead of cepstral space to facilitate easier transformation.</li>
                            <li>A set of Deep Belief Networks were trained to perform the shift of space (and vice versa) and NN to actually perform the conversion in the eigen space.</li>
                        </ul>
                    </section>
                </section>

                <section data-background="#004466">
                    <h3>Traditional vs Modern Systems</h3>
                    <ul>
                        <p>An interesting lesson learnt from this timeline is with regards to the application of systems any ML problem.
                        A couple of decades ago, the major approaches to solve VC began with Vector Quantization (based on k-means clustering) in combination with statistical approaches.
                            Gradually, the approach shifted to Neural Nets and then to GMMs to capture the waveforms.
                            Finally, a more non-linear approach was built using Deep Belief Nets.
                        </p>
                        <p>
                            The above actually prove that at any instance in time, the emerging technologies are very potent in their ability to improve the performance of AI problems that haven't been satisfactorily solved before.
                        </p>
                    </ul>
                </section>

				<section data-background="#cc6600">
					<h2>Current State-of-the-Art</h2>
				</section>

                <section data-background="#004466">
                    <h3>Conception</h3>
                    <ul>
                        <li>The approaches based on GMMs rely on "shallow" voice conversions.<br><p><i>Shallow:</i> methods are based on piecewise linear transformations</p></li>
                        <li>Two Hypothesis:
                            <ol>
                                <li>Human speech is more a non-linear entity, hence, a non-linear VC model is better suited to the problem of VC.</li>
                                <li>Discriminative models are more suited to this task than Generative models.</li>
                            </ol>
                        </li>
                        <li class="fragment">Proof?</li>
                    </ul>
                </section>

                <section data-background="#004466">
                    <h3>Conception (Contd.)</h3>
                    <ul>
                        <li>GMM Based approaches often suffer from over-smoothing due to feature learning through Gaussian mixtures which cause outputs to fall near the mean of the resultant Gaussian.</li>
                        <li>These approaches also suffer from over-fitting as its easy to overshoot the optimum number of Gaussians to overfit training data while training.</li>
                        <li>The work on NN based Voice Conversion (presented in the timeline) is known to perform much better than the GMM based VC.</li>
                        <li>Although NNs may also suffer through problem of over-fitting, it can be avoided by carefully preparing the training data and tweaking hyperparameters.</li>
                    </ul>
                </section>

				<section data-background="#004466">
					<h3>Conception (Contd.)</h3>
					<ul>
						<li>The authors decided to extend their previous work in which they used RBMs to learn higher order features to reduce the problem of over-fitting. A concatenating NN was used as well.</li>
						<li>They improved on the same work to systematically capture temporal as well as latent (deep) relationships between source and target speech by retaining the concatenating NN, but replacing the vanilla RBMs by Speaker Dependent Recurrent Temporal RBMs.</li>
					</ul>
				</section>

				<section data-background="#004466">
					<h3>Let's Get More Technical!</h3>
					<ul>
						<li><div><span><u><i><b>Restricted Boltzmann Machines (RBMs):</b></i></u></span></div></li>
                        <li>Boltzmann Machines are a form of Markov Random Fields whose energy function is of a linear form.</li>
                        <li>Simple BMs are made more powerful by arranging them in such a manner that not all units have an observable component to them i.e. they are arranged in various hidden layers that represent latent behavior that influence the observables but are not themselves directly observable.</li>
					</ul>
				</section>

                <section data-background="#004466">
                    <h3>Technical Background (Contd.)</h3>
                    <ul>
                        <li>These more powerful BMs will become hard to train if all connections are maintained. So, the connections are restricted such that all units from one layer are connected to all other units from another layer. Hence the name "Restricted Boltzmann Machines".</li>
                        <div><div style="width:80%; margin: 0 auto;"><img src="img/bm_rbm.jpg" width="650px" /></div></div>
                    </ul>
                </section>

                <section data-background="#004466">
                    <h3>Technical Background (Contd.)</h3>
                    <p>So how is RBM different from a NN with hidden layers?</p>
                    <p class="fragment">RBMs are also Neural Nets! However, they are not interpreted as feedforward networks, but more like bipartite graphs where the idea is to learn the joint probability distribution of hidden and input variables.</p>
                    <p class="fragment">Not only this, RBMs have a different set of training algorithms. Furthermore, RBMs have the capability to learn representations and generate samples (Generative model), but the trained weights can also be used in the feedforward sense.</p>
                </section>

                <section data-background="#004466">
                    <h3>Building up!</h3>
                    <ul>
 						<li><div><span><u><i><b>Temporal RBMs (TRBMs):</b></i></u></span></div></li>
                        <li>A TRBM is a sequence of RBMs arranged in such a way that at a given timestamp, the RBMs biases depend only on the state of the RBM at the previous timestep.</li>
                        <li>They are analogous to HMMs but with an infinitely large state space and compact emission and transition probabilities. However, they are exponentially expensive to train and hence exact inference is hard.</li>
                        <div><div style="width:50%; margin: 0 auto;"><img src="img/trbm.png" width="400px" /></div></div>
                    </ul>
                </section>

                <section data-background="#004466">
                    <h3>Pre-Heating the Oven!</h3>
                    <ul>
                        <li><div><span><u><i><b>Recurrent Temporal RBMs (RTRBMs):</b></i></u></span></div></li>
                        <li>RTRBMs are an extension to TRBMs that are simple to train and infer from. They are designed much like Recurrent Neural Networks and have a set of weights from hidden of previous timestep to hidden of current timestep which make things easier in terms of the algorithmic design and complexity.</li>
                        <li>RTRBMs are therefore, not only capable of capturing the latent relationships in terms of hidden abstractions, but also can now capture latent temporal relationships, hence making them a good candidate for voice based complex tasks, such as the one for VC.</li>
                    </ul>
                </section>

                <section data-background="#004466">
                    <h3>Recurrent Temporal Restricted Boltzmann Machines</h3>
                    <div>
                        <div style="float:left"><img width="500" src="img/rtrbm.png" /></div>
                        <div style="float:right"><img width="500" src="img/rtrbm_2.png" /></div>
                    </div>
                </section>

                <section data-background="#004466">
                    <h3>Actual Solution</h3>
                    <ul>
                        <li>Encoder-Decoder kind of architecture.</li>
                        <li>The architecture consists of individual Speaker-Dependant RTRBMs.</li>
                        <li>By this, it is meant that the RTRBMs are trained separately on each speaker beforehand (encoder-decoder ends).</li>
                    </ul>
                    <div>
                        <img src="img/archi1.png" align="right" height="250px"/>
                        <span style="padding:10px">1. The feature h<sub>x</sub><sup>(t)</sup> captures linguistic information</span><br/>
                        <span style="padding:10px">2. B, C and h together capture temporal information</span><br/>
                        <span style="padding:10px">3. W and h capture latent abstract information</span><br/>
                    </div>
                </section>

                <section data-background="#004466">
                    <h3>Actual Solution</h3>
                    <ul>
                        <li>Bridge between independent RTRBMs is made up by a Neural Net that maps the linguistic features between the two (as this needs to be kept common across both)</li>
                        <li>Once individual trained components are aligned, the entire flow is fine-tuned using BPTT (Backpropagation Through Time).</li>
                    </ul>
                    <img src="img/archi2.png" align="center" height="250px" width="200px"/>
                </section>

                <section data-background="#004466">
                    <h3>Experimental Results</h3>
                    <ul>
                        <li><b><u>Conditions:</u></b>    4 algorithms used: GMM based, Simple NN based, SD-RBM, RNN and SD-RTRBM.<br/>
                            The 3 NN-based approaches each have 4 layers in total. Conversion types tried were M-F, M-M and F-F.</li>
                        <li><b><u>Objective Evaluation Metric Used: Mel-Cepstral Distortion (MCD)</u></b>   [measure how close the converted vector was to the target vector in the mel-cepstral space]</li>
                        <li><b><u>Subjective Evaluation Metric Used: Mean Opinion Score (MOS)</u></b></li>
                    </ul>
                </section>

                <section data-background="#004466">
                    <h3>Experimental Results (Contd.)</h3>
                    <div class="fragment">
                         <div style="float:left; padding:10px"><img width=400 height=250 src="img/res1.png" /></div>
                         <div style="float:left; padding:10px"><img width=400 src="img/res5.png" /></div>
                    </div>
                    <br/>
                    <div class="fragment">
                        <div style="float:left; padding:10px"><img width=400 height=350 src="img/res2.png" /></div>
                    </div>
                </section>

                <section data-background="#004466">
                    <h3>Experimental Results (Contd.)</h3>
                    <div class="fragment">
                        <div style="float:left; padding:10px"><img width=400 height=350 src="img/res3.png" /></div>
                        <div style="float:left; padding:10px"><img width=400 height=350 src="img/res4.png" /></div>
                    </div>
                </section>

                <section data-background="#004466">
                    <h3>Conclusion</h3>
                    <ul>
                        <li>A state of the art, novel technique for VC was presented, which makes use of SD-RTRBMs and a NN to extract time-dependent unique speaker information from the sequential data.</li>
                        <li>Experiments prove that this new approach is more effective, regardless of gender and in terms of MCD and MOS, than well-known approaches using GMMs, NNs, previous work on RBMs.</li>
                        <li>The new approach can be thought of a special generative type of RNN applied to VC.</li>
                    </ul>
                </section>

                <section data-background="#4d7e65">
                    <h2>Wrapping Up and QA</h2>
                </section>

				<section data-background="#b5533c">
                    <h2>THANK YOU</h2>
					<h1>THE END</h1>
				</section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'concave', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'lib/js/reveal-timeline.js', condition: function(){ return !!document.querySelector( '[data-timeline]' );} } //, js: 'lib/js/timeline-min.js', css: 'lib/css/timeline.css'}
				]
			});

		</script>

	</body>
</html>
